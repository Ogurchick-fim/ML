{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78a7ed04",
   "metadata": {},
   "source": [
    "Atalov S.\n",
    "\n",
    "Fundamentals of Machine Learning and Artificial Intelligence\n",
    "\n",
    "# Gradient Boosted Trees\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff99324",
   "metadata": {},
   "source": [
    "На прошлых занятиях мы научились соединять базовые алгоритмы в ансамбль с помощью бэггинга (и, в частности, строить из решающих деревьев случайные леса). Теперь мы рассмотрим другой способ объединять базовые алгоритмы в композицию – градиентный бустинг.\n",
    "\n",
    "В ходе обучения случайного леса каждый базовый алгоритм строится независимо от остальных. Бустинг, в свою очередь, воплощает идею последовательного построения линейной комбинации алгоритмов. Каждый следующий алгоритм старается уменьшить ошибку текущего ансамбля.\n",
    "\n",
    "Бустинг, использующий деревья решений в качестве базовых алгоритмов, называется градиентным бустингом над решающими деревьями, **Gradient Boosting on Decision Trees, GBDT**. Он отлично работает на выборках с «табличными» данными. Такой бустинг способен эффективно находить нелинейные зависимости в данных различной природы. Этим свойством обладают все алгоритмы, использующие деревья решений, однако именно GBDT обычно выигрывает в подавляющем большинстве задач. Благодаря этому он широко применяется во многих конкурсах по машинному обучению и задачах из индустрии (поисковом ранжировании, рекомендательных системах, таргетировании рекламы, предсказании погоды, пункта назначения такси и многих других).\n",
    "\n",
    "Не так хорошо бустинг проявляет себя на однородных данных: текстах, изображениях, звуке, видео. В таких задачах нейросетевые подходы почти всегда демонстрируют лучшее качество.\n",
    "\n",
    "Хотя деревья решений и являются традиционным выбором для объединения в ансамбли, никто не запрещает использовать и другие алгоритмы, например, линейные модели в качестве базовых (эта возможность реализована в пакете XGBoost). Стоит только понимать, что построенная композиция по сути окажется линейной комбинацией линейных моделей, то есть опять-таки линейной моделью. Это уменьшает возможности ансамбля эффективно определять нелинейные зависимости в данных. В рамках данного курса мы будем рассматривать только бустинг над решающими деревьями."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891d3ea2",
   "metadata": {},
   "source": [
    "## Интуиция\n",
    "Рассмотрим задачу регрессии с квадратичной функцией потерь:\n",
    "$$\n",
    "\\mathcal{L}(y, x) = \\frac{1}{2}\\sum^{N}_{i=1}\\left(y_i -  a(x_i)\\right)^{2} \\rightarrow \\min\n",
    "$$\n",
    "\n",
    "Для решения будем строить композицию из $K$ базовых алгоритмов\n",
    "\n",
    "$$\n",
    "a(x) = a_K(x) = b_1(x) + b_2(x) + \\dots +b_K(x)\n",
    "$$\n",
    "\n",
    "Если мы обучим единственное решающее дерево, то качество такой модели, скорее всего, будет низким. Однако о построенном дереве мы знаем, на каких объектах оно давало точные предсказания, а на каких ошибалось.\n",
    "\n",
    "Попробуем использовать эту информацию и обучим еще одну модель. Допустим, что предсказание первой модели на объекте  $x_l$ на 10 больше, чем необходимо (т.е. $b_1(x_l) = y_l + 10$). Если бы мы могли обучить новую модель, которая на $x_l$ будет выдавать ответ , то сумма ответов этих двух моделей на объекте $x_l$ в точности совпала бы с истинным значением:\n",
    "\n",
    "$$\n",
    "b_1(x_l) + b_2(x_l) = (y_l + 10) + (-10) = y_l\n",
    "$$\n",
    "\n",
    "\n",
    "Другими словами, если вторая модель научится предсказывать разницу между реальным значением и ответом первой, то это позволит уменьшить ошибку композиции.\n",
    "\n",
    "В реальности вторая модель тоже не сможет обучиться идеально, поэтому обучим третью, которая будет «компенсировать» неточности первых двух. Будем продолжать так, пока не построим композицию из $K$  алгоритмов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b125e966",
   "metadata": {},
   "source": [
    "<img src=\"https://yastatic.net/s3/ml-handbook/admin/5_1_848ce5004c.png\" width=600>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56889e7d",
   "metadata": {},
   "source": [
    "Источник:\n",
    "- https://education.yandex.ru/handbook/ml/article/gradientnyj-busting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e314dd0",
   "metadata": {},
   "source": [
    "## Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e86980f",
   "metadata": {},
   "source": [
    "Try Gradient Boosting Classifier. Compare with Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305a584b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b007704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3844ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train = 'https://raw.githubusercontent.com/lobachevksy/teaching/main/titanic/train.csv'\n",
    "df = pd.read_csv(titanic_train)\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
